{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de72ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path setup to resolve package imports (dynamic project root)\n",
    "import sys, os\n",
    "\n",
    "# Derive project root from current notebook directory: <project>/notebook\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd547106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agents.tools.db_tools import db_tool_manager\n",
    "\n",
    "tools = db_tool_manager.get_tools()\n",
    "print(f\"‚úÖ {len(tools)} tools ready\")\n",
    "print(\"Tool names:\", [t.name for t in tools])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cbcab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPARISON NOTEBOOK: OPENAI vs OLLAMA (NO VECTOR DB)\n",
    "\n",
    "import time\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from src.config.prompt import system_prompt, OLLAMA_REACT_PROMPT\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "print(\"‚öîÔ∏è  OPENAI vs OLLAMA (NO VECTOR DB)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 2. AGENT FACTORIES (No tools needed!)\n",
    "def create_openai_agent():\n",
    "    checkpointer = MemorySaver()\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    \n",
    "    agent = create_agent(\n",
    "        model=model,\n",
    "        tools=tools,\n",
    "        system_prompt=system_prompt,\n",
    "        checkpointer=checkpointer\n",
    "    )\n",
    "    return agent, model, checkpointer\n",
    "\n",
    "def create_ollama_agent():\n",
    "    checkpointer = MemorySaver()\n",
    "    model = ChatOllama(model=\"llama3-groq-tool-use\", temperature=0)  # Fast local\n",
    "    \n",
    "    agent = create_agent(\n",
    "        model=model,\n",
    "        tools=tools,\n",
    "        system_prompt=OLLAMA_REACT_PROMPT,\n",
    "        checkpointer=checkpointer\n",
    "    )\n",
    "    return agent, model, checkpointer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebb377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. STREAMING TEST (Same as yours, no vector changes needed)\n",
    "def stream_agent_comparison(agent, model_name, question, thread_id):\n",
    "    \"\"\"Enhanced streaming test with metrics (your original logic)\"\"\"\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ü§ñ {model_name} ‚Üí '{question}'\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    tool_steps = 0  # Will be 0 (no tools)\n",
    "    full_content = \"\"\n",
    "    \n",
    "    for chunk in agent.stream(\n",
    "        {\"messages\": [HumanMessage(content=question)]},\n",
    "        config,\n",
    "        stream_mode=\"values\"\n",
    "    ):\n",
    "        if \"messages\" in chunk and chunk[\"messages\"]:\n",
    "            msg = chunk[\"messages\"][-1]\n",
    "            \n",
    "            # TOOL CALLS \n",
    "            if hasattr(msg, 'tool_calls') and msg.tool_calls:\n",
    "                tool_call = msg.tool_calls[0]\n",
    "                print(f\"üü° [{time.time()-start_time:.1f}s] TOOL: {tool_call['name']}({tool_call['args']})\")\n",
    "                tool_steps += 1\n",
    "            \n",
    "            # TOOL RESULTS\n",
    "            elif hasattr(msg, 'type') and msg.type == \"tool\":\n",
    "                print(f\"üü¢ [{time.time()-start_time:.1f}s] RESULT: {msg.content[:80]}...\")\n",
    "            \n",
    "            # STREAMING CONTENT (main output)\n",
    "            elif msg.content:\n",
    "                full_content += msg.content\n",
    "                print(msg.content, end=\"\", flush=True)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n\\nüìä METRICS: {total_time:.1f}s | Reasoning Steps: {len(full_content.split('.'))} | Chars: {len(full_content)}\")\n",
    "    \n",
    "    # Success = contains SQL keywords\n",
    "    has_sql = any(keyword in full_content.upper() for keyword in [\"SELECT\", \"FROM\", \"WHERE\", \"JOIN\", \"GROUP BY\"])\n",
    "    \n",
    "    return {\n",
    "        \"time\": total_time,\n",
    "        \"reasoning_steps\": len(full_content.split('.')),\n",
    "        \"content_length\": len(full_content),\n",
    "        \"has_sql\": has_sql,\n",
    "        \"content\": full_content.strip(),\n",
    "        \"success\": has_sql and len(full_content) > 50\n",
    "    }\n",
    "\n",
    "# 4. RUN COMPARISON\n",
    "print(\"\\nüèÅ HEAD-TO-HEAD (Pure LLM Reasoning)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create agents\n",
    "openai_agent, _, openai_checkpointer = create_openai_agent()\n",
    "ollama_agent, _, ollama_checkpointer = create_ollama_agent()\n",
    "\n",
    "test_queries = [\n",
    "    \"Show top customers by sales\"\n",
    "]\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\nüìç Query {i+1}/{len(test_queries)}: {query}\")\n",
    "    \n",
    "    # Fresh threads\n",
    "    openai_config = {\"configurable\": {\"thread_id\": f\"no_vec_openai_{i}\"}}\n",
    "    ollama_config = {\"configurable\": {\"thread_id\": f\"no_vec_ollama_{i}\"}}\n",
    "    \n",
    "    # Test both\n",
    "    openai_result = stream_agent_comparison(openai_agent, \"üîµ OPENAI\", query, f\"openai_{i}\")\n",
    "    ollama_result = stream_agent_comparison(ollama_agent, \"üü¢ OLLAMA\", query, f\"ollama_{i}\")\n",
    "    \n",
    "    comparison_results[query] = {\n",
    "        \"openai\": openai_result,\n",
    "        \"ollama\": ollama_result\n",
    "    }\n",
    "\n",
    "# 5. RESULTS TABLE\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"üìä NO-VECTOR RESULTS (Pure LLM Reasoning)\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "print(f\"{'Query':<35} {'OpenAI':<10} {'Ollama':<10} {'OpenAI SQL':<10} {'Ollama SQL':<10} {'Faster':<12} {'Better SQL'}\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "openai_wins, ollama_wins, ties = 0, 0, 0\n",
    "\n",
    "for query, results in comparison_results.items():\n",
    "    openai = results[\"openai\"]\n",
    "    ollama = results[\"ollama\"]\n",
    "    \n",
    "    o_time, ol_time = openai[\"time\"], ollama[\"time\"]\n",
    "    o_sql, ol_sql = openai[\"has_sql\"], ollama[\"has_sql\"]\n",
    "    \n",
    "    # Speed winner\n",
    "    speed_winner = \"üîµ\" if o_time < ol_time else \"üü¢\" if ol_time < o_time else \"ü§ù\"\n",
    "    \n",
    "    # Quality winner (SQL + length)\n",
    "    o_quality = (1 if o_sql else 0) + (openai[\"content_length\"] / 1000)\n",
    "    ol_quality = (1 if ol_sql else 0) + (ollama[\"content_length\"] / 1000)\n",
    "    \n",
    "    if o_quality > ol_quality:\n",
    "        quality_winner = \"üîµ\"\n",
    "        openai_wins += 1\n",
    "    elif ol_quality > o_quality:\n",
    "        quality_winner = \"üü¢\" \n",
    "        ollama_wins += 1\n",
    "    else:\n",
    "        quality_winner = \"ü§ù\"\n",
    "        ties += 1\n",
    "    \n",
    "    print(f\"{query[:34]:<35} {o_time:<9.1f}s {ol_time:<9.1f}s \"\n",
    "          f\"{o_sql:<9} {ol_sql:<9} {speed_winner:<11} {quality_winner}\")\n",
    "\n",
    "# 6. SUMMARY\n",
    "print(\"=\"*60)\n",
    "total_queries = len(test_queries)\n",
    "print(f\"OpenAI SQL success:  {sum(r['openai']['has_sql'] for r in comparison_results.values())}/{total_queries}\")\n",
    "print(f\"Ollama SQL success: {sum(r['ollama']['has_sql'] for r in comparison_results.values())}/{total_queries}\")\n",
    "\n",
    "openai_avg_time = sum(r[\"openai\"][\"time\"] for r in comparison_results.values()) / total_queries\n",
    "ollama_avg_time = sum(r[\"ollama\"][\"time\"] for r in comparison_results.values()) / total_queries\n",
    "\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Average Response Time:\")\n",
    "print(f\"   OpenAI: {openai_avg_time:.1f}s\")\n",
    "print(f\"   Ollama: {ollama_avg_time:.1f}s\")\n",
    "\n",
    "overall_quality_winner = \"üîµ OPENAI\" if openai_wins > ollama_wins else \"üü¢ OLLAMA\"\n",
    "print(f\"\\nüéâ QUALITY WINNER: {overall_quality_winner}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
